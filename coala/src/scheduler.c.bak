#include <msp430.h>
#include <stdint.h>
#include <mspDebugger.h>

#include <scheduler.h>
#include <global.h>
#include <dataprotect.h>
#include <ctlflags.h>
#include <checkpoint.h>

#define PRF_PORT 3
#define PRF_PIN  5

#define pwr_int_count    (*(uint16_t*)(0x1990))
#define pwr_int_overflow (*(uint16_t*)(0x1992))

#define MAX_TARGET_SIZE 0x40 // Max 64 coalesced tasks


// Commit phase.
__nv uint8_t commit_flag = COMMIT_FINISH;

// Task function pointers.
task_pt real_task_addr;
task_pt __next_real_task_addr;
__nv task_pt __coalesced_task_addr;
__nv task_pt next_coalesced_task_addr;

// Checkpointing.
__nv uint8_t checkpoint_available = 0;

// Coalescing.
__nv uint16_t target_size = 1;
__nv uint16_t total_task_count = 10; // TODO: why exactly this value?
uint16_t last_task_count;
uint8_t decrease_target_size = 1;
uint16_t coalesced_task_count = 0;

// Time weight.
uint8_t real_task_time_wgt;
uint8_t __next_real_task_time_wgt;
__nv uint8_t __coalesced_task_time_wgt;
__nv uint8_t next_coalesced_task_time_wgt;

extern ram_page_metadata_t ram_pages_buf[];
extern uint16_t shadow_buf_pages_count;


#if NO_COALESCING

void scheduler()
{
    if (commit_flag == COMMITTING) {
        // Resume from last executing task.
        __coalesced_task_addr = next_coalesced_task_addr;
        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;
    }
    shadow_buf_pages_count = 0;

    // Initialize SRAM working buffer.
    uint16_t page_idx;
    uint16_t page_addr = BGN_RAM;
    for (page_idx = 0; page_idx < RAM_BUF_LEN; page_idx++) {
        ram_pages_buf[page_idx].ram_pg_tag = page_addr;
        page_addr += PAG_SIZE;
    }

    // Recall last executing (coalesced) task.
    real_task_addr = __coalesced_task_addr;

    if (checkpoint_available) {
        Restore();
    }

    // Schedule tasks.
    while (1) {
        // When a coalesced task starts, no previous checkpoint is valid any longer.
        checkpoint_available = 0;

#if PROFILE_SCHEDULER
        msp_gpio_clear(COALA_PRF_PORT, COALA_PRF_SCH_PIN);
#endif

        // Execute task.
        ((task_pt) (real_task_addr))();

#if PROFILE_SCHEDULER
        msp_gpio_set(COALA_PRF_PORT, COALA_PRF_SCH_PIN);
#endif

        // Virtual progressing.
        real_task_addr = __next_real_task_addr;

        // Transition stage (virtual -> persistent).
        next_coalesced_task_addr = real_task_addr;

        // Copy dirty pages to shadow buffer.
        for (page_idx = 0; page_idx < RAM_BUF_LEN && ram_pages_buf[page_idx].fram_pg_tag; page_idx++) {
            if (ram_pages_buf[page_idx].is_dirty) {
                __copy_page_to_shadow_buf(ram_pages_buf[page_idx].ram_pg_tag, ram_pages_buf[page_idx].fram_pg_tag);
                ram_pages_buf[page_idx].is_dirty = PAGE_CLEAN;
            }
        }

        // __persis_CrntPagHeader = CrntPagHeader; // Keep track of the last accessed page over a power cycle

        commit_flag = COMMITTING;

        // Firm transition to next task.
        __coalesced_task_addr = next_coalesced_task_addr;

        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;
    }
}

#elif SLO_CHNG_ALGO

void scheduler()
{
#if NUM_PWR_INTR_MEM_DUMP
    pwr_int_count++;
    if (pwr_int_count == 0) {
        pwr_int_overflow++;
    }
#endif

    // On a power interrupt decrease the target size.
    if (target_size > 1) {
        target_size--;
    }

    if (commit_flag == COMMITTING) {
        // Resume from last executing task.
        __coalesced_task_addr = next_coalesced_task_addr;
        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;
    }
    shadow_buf_pages_count = 0;

    // Initialize SRAM working buffer.
    uint16_t page_idx;
    uint16_t page_addr = BGN_RAM;
    for (page_idx = 0; page_idx < RAM_BUF_LEN; page_idx++) {
        ram_pages_buf[page_idx].ram_pg_tag = page_addr;
        page_addr += PAG_SIZE;
    }

    // Recall last executing (coalesced) task.
    real_task_addr = __coalesced_task_addr;

    if (checkpoint_available) {
        Restore();
    }

    // Schedule tasks.
    while (1) {
        // When a coalesced task starts, no previous checkpoint is valid any longer.
        checkpoint_available = 0;

        // Coalescing.
        coalesced_task_count = 0;

        while (coalesced_task_count < target_size) {

#if PROFILE_SCHEDULER
            msp_gpio_clear(COALA_PRF_PORT, COALA_PRF_SCH_PIN);
#endif

            // Execute task.
            ((task_pt) (real_task_addr))();

#if PROFILE_SCHEDULER
            msp_gpio_set(COALA_PRF_PORT, COALA_PRF_SCH_PIN);
#endif

            // Virtual progressing.
            real_task_addr = __next_real_task_addr;
            // Increment counter.
            coalesced_task_count++;
        }

#if COAL_TSK_SIZ_SEND
        uart_sendHex8(target_size);
        uart_sendStr("\n\r\0");
#endif
        
        // Increase the target size.
        if (target_size < MAX_TARGET_SIZE) {
            target_size++;
        }

        // Transition stage (virtual -> persistent).
        next_coalesced_task_addr = real_task_addr;

        // Copy dirty pages to shadow buffer.
        for (page_idx = 0; page_idx < RAM_BUF_LEN && ram_pages_buf[page_idx].fram_pg_tag; page_idx++) {
            if (ram_pages_buf[page_idx].is_dirty) {
                __copy_page_to_shadow_buf(ram_pages_buf[page_idx].ram_pg_tag, ram_pages_buf[page_idx].fram_pg_tag);
                ram_pages_buf[page_idx].is_dirty = PAGE_CLEAN;
            }
        }

        // __persis_CrntPagHeader = CrntPagHeader; // Keep track of the last accessed page over a power cycle

        commit_flag = COMMITTING;

        // Firm transition to next task.
        __coalesced_task_addr = next_coalesced_task_addr;

        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;
    }
}

#elif FST_CHNG_ALGO

void scheduler()
{
#if NUM_PWR_INTR_MEM_DUMP
    pwr_int_count++;
    if (pwr_int_count == 0) {
        pwr_int_overflow++;
    }
#endif

    // Set the target size as half of the last execution history (plus one)
    // and clear the last execution history (total_task_count).
    target_size = (total_task_count >> 1) + 1;
    total_task_count = 0; // On a power reboot, total_task_count must be reset

    if (commit_flag == COMMITTING) {
        // Resume from last executing task.
        __coalesced_task_addr = next_coalesced_task_addr;
        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;
    }
    shadow_buf_pages_count = 0;

    // Initialize SRAM working buffer.
    uint16_t page_idx;
    uint16_t page_addr = BGN_RAM;
    for (page_idx = 0; page_idx < RAM_BUF_LEN; page_idx++) {
        ram_pages_buf[page_idx].ram_pg_tag = page_addr;
        page_addr += PAG_SIZE;
    }

    // Recall last executing (coalesced) task.
    real_task_addr = __coalesced_task_addr;

    if (checkpoint_available) {
        Restore();
    }

    // Schedule tasks.
    while (1) {
#if COAL_TSK_SIZ_SEND
        uart_sendHex8(coalesced_task_count);
        uart_sendStr("\n\r\0");
#endif
        // When a coalesced task starts, no previous checkpoint is valid any longer.
        checkpoint_available = 0;

        // Coalescing.
        coalesced_task_count = 0;
        while (coalesced_task_count < target_size) {
            // Execute task.
            ((task_pt) (real_task_addr))();
            // Virtual progressing.
            real_task_addr = __next_real_task_addr;
            // Increment counters.
            coalesced_task_count++;
            total_task_count++;
        }

        // Transition stage (virtual -> persistent).
        next_coalesced_task_addr = real_task_addr;

        // Copy dirty pages to shadow buffer.
        for (page_idx = 0; page_idx < RAM_BUF_LEN && ram_pages_buf[page_idx].fram_pg_tag; page_idx++) {
            if (ram_pages_buf[page_idx].is_dirty) {
                __copy_page_to_shadow_buf(ram_pages_buf[page_idx].ram_pg_tag, ram_pages_buf[page_idx].fram_pg_tag);
                ram_pages_buf[page_idx].is_dirty = PAGE_CLEAN;
            }
        }

        // __persis_CrntPagHeader = CrntPagHeader; // Keep track of the last accessed page over a power cycle

        commit_flag = COMMITTING;

        // Firm transition to next task.
        __coalesced_task_addr = next_coalesced_task_addr;

        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;

        target_size = ((total_task_count >> 1) + 1);
        if (target_size > MAX_TARGET_SIZE) {
            target_size = MAX_TARGET_SIZE;
        }
    }
}

#elif FST_CHNG_TSK_AWAR_ALGO

void scheduler()
{
#if NUM_PWR_INTR_MEM_DUMP
    pwr_int_count++;
    if (pwr_int_count == 0) {
        pwr_int_overflow++;
    }
#endif

    // Set the target size as half of the last execution history (plus one)
    // and clear the last execution history (total_task_count).
    target_size = (total_task_count >> 1) + 1;
    total_task_count = 0; // On a power reboot, total_task_count must be reset

    if (commit_flag == COMMITTING) {
        // Resume from last executing task.
        __coalesced_task_addr = next_coalesced_task_addr;
        __coalesced_task_time_wgt = next_coalesced_task_time_wgt;
        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;
    }
    shadow_buf_pages_count = 0;

    // Initialize SRAM working buffer.
    uint16_t page_idx;
    uint16_t page_addr = BGN_RAM;
    for (page_idx = 0; page_idx < RAM_BUF_LEN; page_idx++) {
        ram_pages_buf[page_idx].ram_pg_tag = page_addr;
        page_addr += PAG_SIZE;
    }

    // Recall last executing (coalesced) task.
    real_task_addr = __coalesced_task_addr;
    real_task_time_wgt = __coalesced_task_time_wgt;

    if (checkpoint_available) {
        Restore();
    }

    uint16_t curr_task_time_wgt;

    // Schedule tasks.
    while (1) {
#if COAL_TSK_SIZ_SEND
        uart_sendHex8(coalesced_task_count);
        uart_sendStr("\n\r\0");
#endif
        // When a coalesced task starts, no previous checkpoint is valid any longer.
        checkpoint_available = 0;

        // Coalescing (now coalesced_task_count is in units of risk level).
        coalesced_task_count = 0;
        while (coalesced_task_count < target_size) {
            // Execute task.
            ((task_pt) (real_task_addr))();

            curr_task_time_wgt = real_task_time_wgt;

            // Virtual progressing.
            real_task_addr = __next_real_task_addr;
            real_task_time_wgt = __next_real_task_time_wgt;

            // Increment counters.
            coalesced_task_count += curr_task_time_wgt;
            total_task_count++;
        }

        // Transition stage (virtual -> persistent).
        next_coalesced_task_addr = real_task_addr;
        next_coalesced_task_time_wgt = real_task_time_wgt;

        // Copy dirty pages to shadow buffer.
        for (page_idx = 0; page_idx < RAM_BUF_LEN && ram_pages_buf[page_idx].fram_pg_tag; page_idx++) {
            if (ram_pages_buf[page_idx].is_dirty) {
                __copy_page_to_shadow_buf(ram_pages_buf[page_idx].ram_pg_tag, ram_pages_buf[page_idx].fram_pg_tag);
                ram_pages_buf[page_idx].is_dirty = PAGE_CLEAN;
            }
        }

        // __persis_CrntPagHeader = CrntPagHeader; // Keep track of the last accessed page over a power cycle

        commit_flag = COMMITTING;

        // Firm transition to next task.
        __coalesced_task_addr = next_coalesced_task_addr;
        __coalesced_task_time_wgt = next_coalesced_task_time_wgt;

        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;

        target_size = ((total_task_count >> 1) + 1);
        if (target_size > MAX_TARGET_SIZE) {
            target_size = MAX_TARGET_SIZE;
        }
    }
}

#elif GIVE_ME_A_NAME_ALGO

void scheduler()
{
#if NUM_PWR_INTR_MEM_DUMP
    pwr_int_count++;
    if (pwr_int_count == 0) {
        pwr_int_overflow++;
    }
#endif

    // Set the target size as half of the last execution history (plus one)
    // and clear the last execution history (total_task_count).
    target_size = (total_task_count >> 1) + 1;
    last_task_count = total_task_count;
    total_task_count = 0; // On a power reboot, total_task_count must be reset

    if (commit_flag == COMMITTING) {
        // Resume from last executing task.
        __coalesced_task_addr = next_coalesced_task_addr;
        __coalesced_task_time_wgt = next_coalesced_task_time_wgt;
        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;
    }
    shadow_buf_pages_count = 0;

    // Initialize SRAM working buffer.
    uint16_t page_idx;
    uint16_t page_addr = BGN_RAM;
    for (page_idx = 0; page_idx < RAM_BUF_LEN; page_idx++) {
        ram_pages_buf[page_idx].ram_pg_tag = page_addr;
        page_addr += PAG_SIZE;
    }

    // Recall last executing (coalesced) task.
    real_task_addr = __coalesced_task_addr;
    real_task_time_wgt = __coalesced_task_time_wgt;

    if (checkpoint_available) {
        Restore();
    }

    uint16_t curr_task_time_wgt;

    // Schedule tasks.
    while (1) {
#if COAL_TSK_SIZ_SEND
        uart_sendHex8(coalesced_task_count);
        uart_sendStr("\n\r\0");
#endif
        // When a coalesced task starts, no previous checkpoint is valid any longer.
        checkpoint_available = 0;

        // Coalescing (now coalesced_task_count is in units of risk level).
        coalesced_task_count = 0;
        while (coalesced_task_count < target_size) {
            // Execute task.
            ((task_pt) (real_task_addr))();

            curr_task_time_wgt = real_task_time_wgt;

            // Virtual progressing.
            real_task_addr = __next_real_task_addr;
            real_task_time_wgt = __next_real_task_time_wgt;

            // Increment counters.
            coalesced_task_count += curr_task_time_wgt;
            total_task_count += curr_task_time_wgt;
        }

        // Transition stage (virtual -> persistent).
        next_coalesced_task_addr = real_task_addr;
        next_coalesced_task_time_wgt = real_task_time_wgt;

        // Copy dirty pages to shadow buffer.
        for (page_idx = 0; page_idx < RAM_BUF_LEN && ram_pages_buf[page_idx].fram_pg_tag; page_idx++) {
            if (ram_pages_buf[page_idx].is_dirty) {
                __copy_page_to_shadow_buf(ram_pages_buf[page_idx].ram_pg_tag, ram_pages_buf[page_idx].fram_pg_tag);
                ram_pages_buf[page_idx].is_dirty = PAGE_CLEAN;
            }
        }

        // __persis_CrntPagHeader = CrntPagHeader; // Keep track of the last accessed page over a power cycle

        commit_flag = COMMITTING;

        // Firm transition to next task.
        __coalesced_task_addr = next_coalesced_task_addr;
        __coalesced_task_time_wgt = next_coalesced_task_time_wgt;

        // Copy dirty pages from shadow buffer to store buffer.
        __commit_to_store_buf();
        commit_flag = COMMIT_FINISH;

        // Update target_size
        if (decrease_target_size) {
            if (target_size > 1) {
                target_size = target_size >> 1;
            } else {
                decrease_target_size = 0;
                target_size = target_size << 1;
            }
        } else {
            if (target_size < last_task_count) {
                target_size = target_size << 1;
            } else {
                decrease_target_size = 1;
                target_size = target_size >> 1;
            }
        }
        if (target_size > MAX_TARGET_SIZE) {
            target_size = MAX_TARGET_SIZE;
        }
    }
}

#endif
